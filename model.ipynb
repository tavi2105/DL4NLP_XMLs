{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-24T22:44:12.576687500Z",
     "start_time": "2024-01-24T22:44:12.561073100Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from extr_ds.manager.utils.filesystem import load_document\n",
    "import numpy\n",
    "import evaluate\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, \\\n",
    "    TFAutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "model_checkpoint = 'bert-base-cased'\n",
    "model_output_checkpoint = 'transformers/nfl_pbp_token_classifier'\n",
    "\n",
    "labels = [\n",
    "    \"abbr\",\n",
    "    \"orth\",\n",
    "    \"MorfDef\",\n",
    "    \"gramGrp\",\n",
    "    \"label\",\n",
    "    \"citRange\",\n",
    "    \"quote\",\n",
    "    \"citedRange\",\n",
    "    \"hi\",\n",
    "    \"RegDef\",\n",
    "    \"def\",\n",
    "    \"cit\",\n",
    "    \"term\",\n",
    "    \"norm\",\n",
    "    \"form\",\n",
    "    \"bibl\",\n",
    "    \"usg\"\n",
    "]\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T22:48:34.098551400Z",
     "start_time": "2024-01-24T22:48:34.091624200Z"
    }
   },
   "id": "2bd02f8dbb29c116"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def align_labels(tokenized_inputs, label_list):\n",
    "    labels = []\n",
    "    for word_idx in tokenized_inputs.word_ids(batch_index=0):\n",
    "        label_id = -100\n",
    "        if word_idx is not None:\n",
    "            label = label_list[word_idx]\n",
    "\n",
    "            label_id = label2id[label]\n",
    "\n",
    "        labels.append(label_id)\n",
    "        # previous_word_idx = word_idx\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_dataset(tokenizer, model):\n",
    "    def tokenize_and_align_labels(record):\n",
    "        tokenized_inputs = tokenizer(\n",
    "            record['tokens'],\n",
    "            truncation=True,\n",
    "            is_split_into_words=True\n",
    "        )\n",
    "\n",
    "        tokenized_inputs['labels'] = align_labels(\n",
    "            tokenized_inputs,\n",
    "            record['labels']\n",
    "        )\n",
    "\n",
    "        return tokenized_inputs\n",
    "\n",
    "    ents_dataset = json.loads(\n",
    "        load_document('dataset/dataset.json')\n",
    "    )\n",
    "\n",
    "    # random.shuffle(ents_dataset)\n",
    "\n",
    "    pivot = int(len(ents_dataset) * .8)\n",
    "    data_collator = DataCollatorForTokenClassification(\n",
    "        tokenizer,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "    train_dataset = Dataset.from_list(ents_dataset[:pivot])\n",
    "    tf_train_set = model.prepare_tf_dataset(\n",
    "        train_dataset.map(\n",
    "            tokenize_and_align_labels,\n",
    "            batched=False\n",
    "        ),\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "    test_dataset = Dataset.from_list(ents_dataset[pivot:])\n",
    "    tf_test_set = model.prepare_tf_dataset(\n",
    "        test_dataset.map(\n",
    "            tokenize_and_align_labels,\n",
    "            batched=False\n",
    "        ),\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "    return tf_train_set, tf_test_set\n",
    "\n",
    "\n",
    "seqeval = evaluate.load('seqeval')\n",
    "\n",
    "\n",
    "def compute_metrics(preds):\n",
    "    predictions, actuals = preds\n",
    "    predictions = numpy.argmax(predictions, axis=2)\n",
    "\n",
    "    results = seqeval.compute(\n",
    "        predictions=[\n",
    "            [labels[p] for p, l in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, actuals)\n",
    "        ],\n",
    "        references=[\n",
    "            [labels[l] for p, l in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, actuals)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        key: results[f'overall_{key}']\n",
    "        for key in ['precision', 'recall', 'f1', 'accuracy']\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T22:48:42.129123400Z",
     "start_time": "2024-01-24T22:48:40.498307800Z"
    }
   },
   "id": "21e851d7b15ff503"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/529 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be8b185f50b943c4a846b6ab39ce0f4c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/133 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5e5630db5a374a9a9a234aba84e9caa9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_checkpoint\n",
    ")\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "tf_train_set, tf_test_set = get_dataset(tokenizer, model)\n",
    "\n",
    "callbacks = [\n",
    "    KerasMetricCallback(\n",
    "        metric_fn=compute_metrics,\n",
    "        eval_dataset=tf_test_set\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "]\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "model.compile(optimizer=optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T22:48:48.019148200Z",
     "start_time": "2024-01-24T22:48:42.737901900Z"
    }
   },
   "id": "fe0df450ce6bebfe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "66/66 [==============================] - 1276s 19s/step - loss: 1.3206 - val_loss: 1.6987 - precision: 0.2136 - recall: 0.2864 - f1: 0.2447 - accuracy: 0.3733\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 1258s 19s/step - loss: 0.4523 - val_loss: 2.0068 - precision: 0.3189 - recall: 0.3717 - f1: 0.3433 - accuracy: 0.3986\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 1355s 21s/step - loss: 0.2961 - val_loss: 1.2717 - precision: 0.2668 - recall: 0.3779 - f1: 0.3128 - accuracy: 0.4453\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 1294s 20s/step - loss: 0.1862 - val_loss: 1.2803 - precision: 0.2330 - recall: 0.3818 - f1: 0.2894 - accuracy: 0.5391\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 1297s 19s/step - loss: 0.1281 - val_loss: 2.0234 - precision: 0.3567 - recall: 0.4251 - f1: 0.3879 - accuracy: 0.4118\n",
      "Epoch 6/10\n",
      "18/66 [=======>......................] - ETA: 11:39 - loss: 0.1009"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=tf_train_set,\n",
    "    validation_data=tf_test_set,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-24T22:49:49.758955900Z"
    }
   },
   "id": "9bdf913b2503131c"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "for model_to_save in [tokenizer, model]:\n",
    "  model_to_save.save_pretrained(model_output_checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T22:45:21.313809100Z",
     "start_time": "2024-01-24T22:45:17.978325500Z"
    }
   },
   "id": "787380a3b44eae04"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at transformers/nfl_pbp_token_classifier were not used when initializing TFBertForTokenClassification: ['dropout_75']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at transformers/nfl_pbp_token_classifier.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'entity_group': 'orth', 'score': 0.9527491, 'word': 'DISFAVORITOR, - OÂRE', 'start': 0, 'end': 19}, {'entity_group': 'gramGrp', 'score': 0.672104, 'word': 'adj', 'start': 20, 'end': 23}, {'entity_group': 'MorfDef', 'score': 0.64761406, 'word': '.', 'start': 23, 'end': 24}, {'entity_group': 'usg', 'score': 0.8015866, 'word': '( învechit, rar )', 'start': 25, 'end': 40}, {'entity_group': 'RegDef', 'score': 0.8467941, 'word': 'Care defavorizează.', 'start': 41, 'end': 60}, {'entity_group': 'hi', 'score': 0.5822195, 'word': 'A', 'start': 61, 'end': 62}, {'entity_group': 'RegDef', 'score': 0.41776785, 'word': 'tract', 'start': 63, 'end': 68}, {'entity_group': 'hi', 'score': 0.570468, 'word': '##arisi şi a - şi dobândi oarecare tocmele mai puţin disfavoritoare.', 'start': 68, 'end': 132}, {'entity_group': 'abbr', 'score': 0.7345371, 'word': 'AR ( 1829 ), 1071 / 34.', 'start': 133, 'end': 152}, {'entity_group': 'form', 'score': 0.76842046, 'word': '- Pl. :', 'start': 157, 'end': 163}, {'entity_group': 'hi', 'score': 0.7159457, 'word': 'disfavoritori, - oare. — De la disfavoare.', 'start': 164, 'end': 205}]]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\n",
    "    'ner',\n",
    "    model=model_output_checkpoint,\n",
    "    aggregation_strategy='simple'\n",
    ")\n",
    "\n",
    "examples = [\n",
    "  'DISFAVORITOR, -OÂRE adj. (învechit, rar) Care defavorizează. A tractarisi şi a-şi dobândi oarecare tocmele mai puţin disfavoritoare. AR (1829), 1071/34.     - Pl.: disfavoritori, -oare. — De la disfavoare.',\n",
    "]\n",
    "\n",
    "responses = classifier(examples)\n",
    "print(responses)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T22:45:32.492045Z",
     "start_time": "2024-01-24T22:45:27.618186400Z"
    }
   },
   "id": "9d83bfdde5aa0f9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c06efe8fadc872a1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
