{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-25T02:00:14.616859200Z",
     "start_time": "2024-01-25T01:59:59.669029800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\FII\\DL4NLP\\Project\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from extr_ds.manager.utils.filesystem import load_document\n",
    "import numpy\n",
    "import evaluate\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, \\\n",
    "    TFAutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "model_checkpoint = 'bert-base-cased'\n",
    "model_output_checkpoint = 'transformers/nfl_pbp_token_classifier'\n",
    "\n",
    "labels = [\n",
    "    \"abbr\",\n",
    "    \"orth\",\n",
    "    \"MorfDef\",\n",
    "    \"gramGrp\",\n",
    "    \"label\",\n",
    "    \"citRange\",\n",
    "    \"quote\",\n",
    "    \"citedRange\",\n",
    "    \"hi\",\n",
    "    \"RegDef\",\n",
    "    \"def\",\n",
    "    \"cit\",\n",
    "    \"term\",\n",
    "    \"norm\",\n",
    "    \"form\",\n",
    "    \"bibl\",\n",
    "    \"usg\"\n",
    "]\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T02:00:14.632357300Z",
     "start_time": "2024-01-25T02:00:14.616859200Z"
    }
   },
   "id": "2bd02f8dbb29c116"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def align_labels(tokenized_inputs, label_list):\n",
    "    labels = []\n",
    "    for word_idx in tokenized_inputs.word_ids(batch_index=0):\n",
    "        label_id = -100\n",
    "        if word_idx is not None:\n",
    "            label = label_list[word_idx]\n",
    "\n",
    "            label_id = label2id[label]\n",
    "\n",
    "        labels.append(label_id)\n",
    "        # previous_word_idx = word_idx\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_dataset(tokenizer, model):\n",
    "    def tokenize_and_align_labels(record):\n",
    "        tokenized_inputs = tokenizer(\n",
    "            record['tokens'],\n",
    "            truncation=True,\n",
    "            is_split_into_words=True\n",
    "        )\n",
    "\n",
    "        tokenized_inputs['labels'] = align_labels(\n",
    "            tokenized_inputs,\n",
    "            record['labels']\n",
    "        )\n",
    "\n",
    "        return tokenized_inputs\n",
    "\n",
    "    ents_dataset = json.loads(\n",
    "        load_document('dataset/dataset.json')\n",
    "    )\n",
    "\n",
    "    # random.shuffle(ents_dataset)\n",
    "\n",
    "    pivot = int(len(ents_dataset) * .8)\n",
    "    data_collator = DataCollatorForTokenClassification(\n",
    "        tokenizer,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "    train_dataset = Dataset.from_list(ents_dataset[:pivot])\n",
    "    tf_train_set = model.prepare_tf_dataset(\n",
    "        train_dataset.map(\n",
    "            tokenize_and_align_labels,\n",
    "            batched=False\n",
    "        ),\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "    test_dataset = Dataset.from_list(ents_dataset[pivot:])\n",
    "    tf_test_set = model.prepare_tf_dataset(\n",
    "        test_dataset.map(\n",
    "            tokenize_and_align_labels,\n",
    "            batched=False\n",
    "        ),\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "    return tf_train_set, tf_test_set\n",
    "\n",
    "\n",
    "seqeval = evaluate.load('seqeval')\n",
    "\n",
    "\n",
    "def compute_metrics(preds):\n",
    "    predictions, actuals = preds\n",
    "    predictions = numpy.argmax(predictions, axis=2)\n",
    "\n",
    "    results = seqeval.compute(\n",
    "        predictions=[\n",
    "            [labels[p] for p, l in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, actuals)\n",
    "        ],\n",
    "        references=[\n",
    "            [labels[l] for p, l in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, actuals)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        key: results[f'overall_{key}']\n",
    "        for key in ['precision', 'recall', 'f1', 'accuracy']\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T02:00:16.022265100Z",
     "start_time": "2024-01-25T02:00:14.638425800Z"
    }
   },
   "id": "21e851d7b15ff503"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\FII\\DL4NLP\\Project\\venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/529 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "806c67318535471da2855b243cdccd85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/133 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa1a80aadaf24189937da70ac4457b19"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_checkpoint\n",
    ")\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "tf_train_set, tf_test_set = get_dataset(tokenizer, model)\n",
    "\n",
    "callbacks = [\n",
    "    KerasMetricCallback(\n",
    "        metric_fn=compute_metrics,\n",
    "        eval_dataset=tf_test_set\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "]\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "model.compile(optimizer=optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T02:00:21.561141400Z",
     "start_time": "2024-01-25T02:00:16.024483500Z"
    }
   },
   "id": "fe0df450ce6bebfe"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From D:\\FII\\DL4NLP\\Project\\venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "66/66 [==============================] - 1079s 16s/step - loss: 1.2768 - val_loss: 1.3172 - precision: 0.1869 - recall: 0.3007 - f1: 0.2305 - accuracy: 0.4266\n",
      "Epoch 2/3\n",
      "66/66 [==============================] - 1146s 18s/step - loss: 0.4586 - val_loss: 1.5531 - precision: 0.2880 - recall: 0.3722 - f1: 0.3248 - accuracy: 0.3952\n",
      "Epoch 3/3\n",
      "66/66 [==============================] - 1207s 19s/step - loss: 0.3090 - val_loss: 1.1509 - precision: 0.2188 - recall: 0.3737 - f1: 0.2760 - accuracy: 0.5142\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.src.callbacks.History at 0x1828308aa40>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=tf_train_set,\n",
    "    validation_data=tf_test_set,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T02:57:34.214180500Z",
     "start_time": "2024-01-25T02:00:21.570708900Z"
    }
   },
   "id": "9bdf913b2503131c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "for model_to_save in [tokenizer, model]:\n",
    "  model_to_save.save_pretrained(model_output_checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T02:57:37.083707100Z",
     "start_time": "2024-01-25T02:57:34.116710500Z"
    }
   },
   "id": "787380a3b44eae04"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at transformers/nfl_pbp_token_classifier were not used when initializing TFBertForTokenClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at transformers/nfl_pbp_token_classifier.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'entity_group': 'orth', 'score': 0.98880124, 'word': 'DISFAVORITOR, - OÂRE', 'start': 0, 'end': 19}, {'entity_group': 'gramGrp', 'score': 0.9227743, 'word': 'adj', 'start': 20, 'end': 23}, {'entity_group': 'MorfDef', 'score': 0.9345763, 'word': '.', 'start': 23, 'end': 24}, {'entity_group': 'usg', 'score': 0.9205247, 'word': '( învechit, rar )', 'start': 25, 'end': 40}, {'entity_group': 'RegDef', 'score': 0.80014676, 'word': 'Care defavorizează. A tractarisi şi a', 'start': 41, 'end': 78}, {'entity_group': 'quote', 'score': 0.35433328, 'word': '-', 'start': 78, 'end': 79}, {'entity_group': 'RegDef', 'score': 0.49321067, 'word': 'şi dobândi oarecare tocmele mai puţin disfavoritoare.', 'start': 79, 'end': 132}, {'entity_group': 'abbr', 'score': 0.89284194, 'word': 'AR ( 1829 ), 1071 / 34.', 'start': 133, 'end': 152}, {'entity_group': 'form', 'score': 0.90721315, 'word': '- Pl. : dis', 'start': 157, 'end': 167}, {'entity_group': 'hi', 'score': 0.72690976, 'word': '##favoritori,', 'start': 167, 'end': 178}, {'entity_group': 'form', 'score': 0.48898032, 'word': '-', 'start': 179, 'end': 180}, {'entity_group': 'hi', 'score': 0.8258106, 'word': 'oare.', 'start': 180, 'end': 185}, {'entity_group': 'form', 'score': 0.97530746, 'word': '— De la disfavoare.', 'start': 186, 'end': 205}]]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\n",
    "    'ner',\n",
    "    model=model_output_checkpoint,\n",
    "    aggregation_strategy='simple'\n",
    ")\n",
    "\n",
    "examples = [\n",
    "  'DISFAVORITOR, -OÂRE adj. (învechit, rar) Care defavorizează. A tractarisi şi a-şi dobândi oarecare tocmele mai puţin disfavoritoare. AR (1829), 1071/34.     - Pl.: disfavoritori, -oare. — De la disfavoare.',\n",
    "]\n",
    "\n",
    "responses = classifier(examples)\n",
    "print(responses)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-25T02:57:40.454539100Z",
     "start_time": "2024-01-25T02:57:37.106995Z"
    }
   },
   "id": "9d83bfdde5aa0f9b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c06efe8fadc872a1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
